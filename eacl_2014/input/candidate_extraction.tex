\section{Introduction}
\label{sec:section}
  Keyphrases are single or multi-word expressions that represent the main topics
  or concepts addressed in a document. Keyphrases are useful in many tasks such
  as document summarization~\cite{avanzo2005keyphrase}, document
  clustering~\cite{han2007webdocumentclustering}, information
  retrieval~\cite{medelyan2008smalltrainingset} or multi-sentence
  compression~\cite{boudin2013multisentencecompression}.
  Since the last decade, information mediums, such as the Internet, give us an
  access to huge amounts of documents. To ease their retrieval and to provide an
  overview of them for end users, the mentioned tasks are needed. However,
  keyphrases are not always associated to documents. To tackle this problem,
  researchers automatically extract keyphrases.

  The automatic keyphrase extraction task consists in the extraction of the
  textual units that represent the main topics, or concepts, of a document.
  Automatic keyphrase extraction methods are divided into two
  categories: supervised and unsupervised methods. Supervised methods typically
  recast the keyphrase extraction task as a binary classification
  task~\cite{witten1999kea}. For unsupervised methods, the keyphrase extraction
  task is often considered as a ranking task and different approaches are
  proposed~\cite{hassan2010conundrums}.

  Although supervised and unsupervised methods handle the keyphrase extraction
  problem differently, they rely on the same preprocessing steps. Documents are
  preprocessed to add linguistic knowledge, such as Part-Of-Speech (POS), and
  the textual units that fit predefined keyphrase properties are extracted as
  keyphrase candidates. In this paper, we study the impact of the candidate
  extraction step on the keyphrase extraction task. This step is the most
  critical step because only keyphrase candidates can be extracted as
  keyphrases.

  Various methods are commonly employed to extract keyphrase candidates.
  Usually, the methods extract filtered n-grams, NP-chunks or word sequences
  matching given patterns~\cite{hulth2003keywordextraction}. Also, some
  researchers, such as \newcite{kim2009reexaminingautomatickeyphraseextraction},
  propose more specific keyphrase candidates using statistical measures.
  However, no previous work compares the existing methods and their impact on
  different categories of keyphrase extraction methods. We do this by
  confronting the number of extracting candidates to the maximum recall that
  can be achieved and by comparing the results of keyphrase extraction methods
  obtained with the candidate extraction methods.

  Another contribution of our work is the use of terminological phrases (terms)
  as keyphrase candidates. Terms are grammatically arranged sequences of
  words designating a concept and treated as single units. Hence, we believe
  that terms are suitable keyphrase candidates.

  This paper is organized as follows.
  Section~\ref{sec:definition_of_candidate_keyphrases} defines the properties of
  keyphrases, Section~\ref{sec:candidate_extraction} presents the methods for
  the extraction of keyphrase candidates and
  Section~\ref{sec:keyphrase_extraction} describes the three keyphrase
  extraction methods used in our experiments, which are presented in
  Section~\ref{sec:evaluation}. Finally, Section~\ref{sec:conclusion} concludes
  this work and discusses future work.

\section{What is a Keyphrase?}
\label{sec:definition_of_candidate_keyphrases}
  In this section, we determine the syntactic properties of a keyphrase. To do
  that, we analyze the reference keyphrases provided with three standard
  evaluation datasets.

  \subsection{Datasets}
  \label{subsec:keyphrase_extraction_datasets}
    Keyphrase extraction datasets are collections of documents paired with
    reference keyphrases given by authors, readers or both. In our work, we use
    three standard datasets that differ in terms of document size,  type and
    language.

    The \textbf{DUC} dataset \cite{over2001duc} is a collection of 308 English
    news articles covering about 30 topics (e.g.~tornadoes, gun control, etc.).
    This collection is the test dataset of the DUC-2001 summarization evaluation
    campaign. This part of DUC-2001 contains reference keyphrases annotated by
    \newcite{wan2008expandrank}. We split the collection into two sets: a
    training set containing 208 documents and a test set containing 100
    documents.

    The \textbf{SemEval} dataset \cite{kim2010semeval} contains 284 English
    scientific papers collected from the ACM Digital Libraries (conference and
    workshop papers). The papers are divided into three sets: a trial set
    containing 40 documents (unused in this work), a training set containing 144
    documents and a test set containing 100 documents. The associated keyphrases
    are provided by both authors and readers.

    The \textbf{DEFT} dataset \cite{Paroubek2012deft} is a collection of 234
    French scientific papers belonging to the \textit{Humanities and Social
    Sciences} domain. DEFT is divided into two sets: a training set containing
    141 documents and a test set containing 93 documents. Keyphrases provided
    with the documents of DEFT are given by authors.

  \subsection{Analysis of Reference Keyphrases}
  \label{subsec:keyphrase_analysis}
    In this section, we aim to find the syntactic properties of most keyphrases,
    for English and French, by analyzing the training sets of the datasets. As
    the datasets have different characteristics (size, type, keyphrase
    annotators, etc.), the inferred properties are general keyphrase properties.

    Table~\ref{tab:train_dataset_statistics} shows statistics about the dataset
    documents and their associated keyphrases. First, keyphrases are presented
    regarding their number of words, which is a clue about their degree of
    conciseness. Second, the keyphrases are presented regarding the
    Part-Of-Speech of their words. To obtain these Part-Of-Speech, we
    automatically POS tagged the keyphrases with the Stanford POS
    tagger~\cite{toutanova2003stanfordpostagger} for English and
    MElt~\cite{denis2009melt} for French. To avoid tagging errors, the given POS
    tags were manually corrected.
    \begin{table}[h]
      \centering
      \begin{tabular}{@{~}r@{~}r@{~}c@{~}c@{~}c@{~}}
        \toprule
        & \multirow{2}{*}[-2pt]{\textbf{Statistic}} & \multicolumn{3}{c}{\textbf{Corpus}}\\
        \cmidrule{3-5}
        & & DUC & SemEval & DEFT\\
        \midrule
        \multirow{6}{*}[-2pt]{\begin{sideways}\textbf{Documents}\end{sideways}} & Language & English & English & French\\
        & Type & News & Papers & Papers\\
        & Number & 208 & 144 & 141\\
        & Token average & 912.0 & 5,134.6 & 7,276.7\\
        & Keyphrase average & 8.1 & 15.4 & 5.4\\
        & Missing keyphrases & 3.9\% & 13.5\% & 18.2\%\\
        \addlinespace[1.5\defaultaddspace]
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{Keyphrases}\end{sideways}} & Unigrams & 17.1\% & 20.2\% & 60.2\%\\
        & Bigrams & 60.8\% & 53.4\% & 24.5\%\\
        & Trigrams & 17.8\% & 21.3\% & $~~$8.8\%\\
        & Quadrigrams & $~~$3.0\% & $~~$3.9\% & $~~$4.2\%\\
        & N-grams (N $\geq$ 5) & $~~$1.3\% & $~~$1.3\% & $~~$2.4\%\\
        \addlinespace[1.5\defaultaddspace]
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{Multi-word keyphrases}\end{sideways}} & Cont. nouns & 94.5\% & 98.7\% & 93.3\%\\
        & Cont. proper nouns & 17.1\% & $~~$4.3\% & $~~$6.9\%\\
        & Cont. adjectives & 50.0\% & 50.2\% & 65.5\%\\
        & Cont. verbs & $~~$1.0\% & $~~$4.0\% & $~~$1.0\%\\
        & Cont. adverbs & $~~$1.6\% & $~~$0.7\% & $~~$1.3\%\\
        & Cont. prepositions & $~~$0.3\% & $~~$1.5\% & 31.2\%\\
        & Cont. determiners & $~~$0.0\% & $~~$0.0\% & 20.4\%\\
        & Cont. others & $~~$1.5\% & $~~$2.5\% & 11.8\%\\
        \addlinespace[.5\defaultaddspace]
        \bottomrule
      \end{tabular}
      \caption{Statistics of the training datasets.
               %As a matter of consistency regarding
               %the evaluation of keyphrase extraction methods, the missing
               %keyphrases are determined based on the stemmed forms.
               \label{tab:train_dataset_statistics}}
    \end{table}

    \begin{table*}
      \centering
      \begin{tabular}{@{~}rl@{~~}l@{~~}l@{~~}ll@{~}}
        \toprule
        & \multicolumn{4}{l}{\hspace{-.5em}\textbf{Pattern}} & \textbf{Example}\\
        \midrule
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{English}\end{sideways}} & Nc & Nc & & & \textit{``hurricane expert''}\\ % AP880409-0015
        & A & Nc & & & \textit{``turbulent summer''}\\ % AP88049-0015
        & Nc & & & & \textit{``storms''}\\ % AP880409-0015
        & A & Nc & Nc & & \textit{``annual hurricane forecast''}\\ % AP880409-0015
        & Nc & Nc & Nc & & \textit{``hurricane reconnaissance flights''}\\ % AP890529-0030
        \addlinespace[1.5\defaultaddspace]
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{French}\end{sideways}} & Nc & & & & \textit{``patrimoine'' (``cultural heritage'')}\\ % as_2002_007048ar
        & Nc & A & & & \textit{``tradition orale'' (``oral tradition'')}\\ % as_2002_007048ar
        & Np & & & & \textit{``Indonésie'' (``Indonesia'')}\\ % as_2001_000235ar
        & Nc & Sp & D & Nc & \textit{``conservation de la nature'' (``nature conservation'')}\\ % as_2005_011742ar
        & Nc & Sp & Nc & & \textit{``traduction en anglais'' (``English translation'')}\\ % meta_2003_006958ar
        \bottomrule
      \end{tabular}
      \caption{Frequent part-of-speech patterns (Multex format) for English and
               French keyphrases. \label{tab:best_patterns}}
    \end{table*}

    From Table~\ref{tab:train_dataset_statistics}, we observe that most of the
    keyphrases are unigrams or bigrams ($\simeq$~$80\%$). Keyphrases are more
    often single words or concise expressions.
    
    \begin{property}\label{prop:informativity}
      Keyphrases bear the minimum information representing an important topic or
      concept (e.g.~``T-2 Buckeye'' instead of ``two-seat T-2 Buckeye'').
    \end{property}

    Table~\ref{tab:train_dataset_statistics} shows the ratio of keyphrases that
    contain specific POS tags. We observed that keyphrases containing one word
    are mostly nouns or proper nouns. Hence, we only show the ratios for the
    multi-word keyphrases. In both English and French, almost every keyphrase
    contain nouns and half of the keyphrases are modified by one or more
    adjectives. In French, we observe that the usage of prepositions and
    determiners is more frequent that in English. In English, prepositional
    expressions are often replaced by non-prepositional variants, but such
    variants cannot be generated in French. Also, unexpected Part-Of-Speech, such
    as foreign words, are not rare in the DEFT dataset, which contains French
    scientific papers that refer to English technical terms.

    \begin{property}\label{prop:noun_phrases}
      Keyphrases are mostly nouns (e.g.~``storms'') that can be modified by one
      or more adjectives (e.g.~``\underline{annual} hurricane forecast'').
    \end{property}

    To give an idea of the observed syntactic patterns,
    Table~\ref{tab:best_patterns} shows the five most frequent POS tag patterns
    for English and French.

\section{Candidate Extraction}
\label{sec:candidate_extraction}
  The aim of the candidate extraction is to determine the textual units that
  could be extracted as keyphrases. This step removes noise, i.e.~irrelevant
  textual units that may deteriorate the performance of the keyphrase
  extraction, and reduces the computation time required to extract the
  keyphrases. We distinguish two categories of candidates: the positive
  candidates, which match reference keyphrases, and the off-target candidates,
  which do not match reference keyphrases. Among the off-target candidates, we
  also distinguish the information bearing candidates, which can help to improve
  the keyphrase extraction, and the irrelevant candidates, which are common
  expressions or belong to an inter-/transdisciplinary lexicon.

  In this section, we present the textual units that are commonly used as
  candidates and we propose the terminological phrases as keyphrase candidates.
  We also discuss their consistency regarding the properties inferred in
  Section~\ref{sec:definition_of_candidate_keyphrases}.

  \paragraph{N-grams} are ordered sequences of $n$ words. In previous work,
  every word sequence of size $n = \{1..3\}$ are
  extracted~\cite{witten1999kea,turney1999learningalgorithms,hulth2003keywordextraction}.
  Extracting n-grams is the best way to obtain as much positive or information
  bearing candidates as possible, but it also gives many irrelevant candidates.
  \newcite{witten1999kea} propose to extract only keyphrases that do not
  contain a stop word (conjunction, preposition, determiner or common word) at
  its beginning or end. Although textual units are filtered by stop words, this
  candidate extraction method is exhaustive and does not strictly fit
  properties~\ref{prop:informativity} and~\ref{prop:noun_phrases}.

  \paragraph{Textual units matching given POS tag patterns} are textual units of
  specific syntactic forms. Extracting such textual units have the benefit to
  ensure grammaticality and to precisely define the nature of the
  candidates. In previous work, \newcite{hulth2003keywordextraction} experiments
  with the most frequent POS tag patterns of her training data\footnote{Frequent
  patterns are the ones that appear at least ten times in the reference.},
  whereas other researchers extract the longest sequences of nouns, proper nouns
  and adjectives, namely the longest NPs~\cite{hassan2010conundrums}. Both
  approaches fit Property~\ref{prop:noun_phrases}, but not always
  Property~\ref{prop:informativity}. Also, the first approach requires training
  data, whereas the second one defines a more generic pattern that requires a
  limited adaptation to different languages.

  \paragraph{NP-chunks} are non-recursive noun phrases.
  \newcite{hulth2003keywordextraction} uses them in her experiments and argues
  that they are less arbitrary and more linguistically justified than other
  candidates, such as n-grams. Also, as NP-chunks are minimal noun phrases,
  they are consistent with both properties~\ref{prop:informativity}
  and~\ref{prop:noun_phrases}.

  \paragraph{Terminological phrases (terms)} are word sequences designating a
  concept and treated as single units for a specific domain. Since a keyphrase
  represents a topic or a concept, it seems relevant to extract
  keyphrases from  a set of terminological phrases. Besides, extracting
  terminological phrases as candidates is linguistically justified and not
  arbitrary. Also, terms are often extracted using POS tag
  patterns consistent with properties~\ref{prop:informativity}
  and~\ref{prop:noun_phrases}~\cite{castellvi2001automatictermdetection}.

%  In their review of automatic term extraction methods,
%  \newcite{castellvi2001automatictermdetection} stated that alongside term
%  extraction there is the document indexing task. Indeed, the textual units that
%  index a given document, i.e.~best describe its content, often are
%  terminological phrases. Hence, we believe that the complex noun phrase
%  sub-compounding method of \newcite{evans1996nounphraseanalysis} is suitable
%  for keyphrase candidate extraction. The aim of their method is to extract both
%  complex noun phrases (e.g.~\textit{``the quality of surface of treated
%  stainless steel strip''}) and their meaningful sub-compounds. Four types of
%  sub-compounds are defined:
%  \begin{enumerate}
%    \item{Lexical atoms, i.e.~semantically coherent phrases
%          (e.g.~\textit{``stainless steel''})
%          \label{item:lexical_atom}}
%    \item{Head modifier pairs (e.g.~\textit{``treated steel''})
%          \label{item:head_modifier}}
%    \item{Cross preposition modification pairs (e.g.~\textit{``surface
%          quality''})
%          \label{item:cross_preposition_modifier}}
%    \item{Sub-compounds (e.g.~\textit{``stainless steel strip''})
%          \label{item:subcompound}}
%  \end{enumerate}
%  The \ref{item:lexical_atom}$^\text{st}$ and the
%  \ref{item:subcompound}$^\text{th}$ types represent continuous pairs, whereas
%  the \ref{item:head_modifier}$^\text{nd}$ and the
%  \ref{item:cross_preposition_modifier}$^\text{rd}$ types can be both continuous
%  or discontinuous pairs. In our work, we do not consider the head modifiers
%  (\ref{item:head_modifier}) and the cross preposition modification pairs
%  (\ref{item:cross_preposition_modifier}), because we aim to only extract
%  keyphrases that appear continuously in the analyzed document.

\section{Keyphrase Extraction}
\label{sec:keyphrase_extraction}
  Once keyphrase candidates are extracted, the second step of the keyphrase
  extraction task is to classify them as ``keyphrase'', or ``non-keyphrase'', or
  rank them and select the $k$ bests as keyphrases. Usually, the classification
  is performed by supervised methods, whereas the ranking is performed by
  unsupervised methods. Supervised methods are usually more efficient than
  unsupervised methods because they learn what are keyphrases, based on
  annotated data.

  In this section, we detail the three keyphrase extraction methods that we use
  in our study.

  \paragraph{TF-IDF~\textnormal{\cite{jones1972tfidf}}} is a weighting scheme
  that represents the significance of a word in a given document. A significant
  word must be both frequent in the document and specific to it. The specificity
  of a word is determined based on its appearance within the documents of a
  given collection. The intuition is that the lower is the amount of documents
  containing a word, the higher is the specificity of the word.

  The keyphrase candidates are scored according to the sum of the TF-IDF weights
  of their words and the $k$ candidates with the highest scores are extracted as
  keyphrases.

  \paragraph{TopicRank~\textnormal{\cite{bougouin2013topicrank}}} aims to
  extract keyphrases that best represent the main topics of a document.
  Keyphrase candidates are clustered into topics using a Jaccard similarity and
  a Hierarchical Agglomerative Clustering, each topic is scored using the
  TextRank random walk~\cite{mihalcea2004textrank} and one representative
  keyphrase from each of the $k$ best ranked topics is extracted.

  \paragraph{KEA~\textnormal{\cite{witten1999kea}}} is a supervised method that
  uses a Naive Bayes classifier to extract keyphrases. The classifier combines
  two feature probabilities to give the class ``keyphrase'', or
  ``non-keyphrase'', to a candidate. The two features are the TF-IDF
  weight\footnote{The TF-IDF weight computed for KEA is based on candidate
  frequency, not word frequency.} of the candidate and the position of its first
  appearance in the document.

\section{Experiments}
\label{sec:evaluation}
  To observe the impact of the candidate extraction methods, we perform two
  experiments. We compare the quality of the candidate extraction methods using
  an intrinsic experiment and we compare their impact on the keyphrase
  extraction task, applying them to TF-IDF, TopicRank and KEA.

  \subsection{Experimental Settings}
  \label{subsec:evaluation_settings}
    \subsubsection{Datasets}
    \label{subsubsec:datasets}
      The datasets used to evaluate the methods are the test sets of DUC,
      SemEval and DEFT (see Section~\ref{subsec:keyphrase_extraction_datasets}).
      Table~\ref{tab:test_dataset_statistics} reports the statistics extracted
      from these test sets.
      \begin{table}
        \centering
        \begin{tabular}{@{~}r@{~}c@{~}c@{~}c@{~}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Statistic}} & \multicolumn{3}{c}{\textbf{Corpus}}\\
          \cmidrule{2-4}
          & DUC & SemEval & DEFT\\
          \midrule
          Language & English & English & French\\
          Type & News & Papers & Papers\\
          Documents & 100 & 100 & 93\\
          Token average & 877.3 & 5,177.7 & 6,839.4\\
          Keyphrase average & 7.9 & 14.7 & 5.2\\
          Tokens/keyphrase & 2.1 & 2.1 & 1.6\\
          Missing keyphrases & 2.8\% & 22.1\% & 21.1\% \\
          \bottomrule
        \end{tabular}
        \caption{Statistics of the test datasets.
                 %As a matter of consistency regarding
                 %the evaluation of keyphrase extraction methods, the missing
                 %keyphrases are determined based on the stemmed forms.
                 \label{tab:test_dataset_statistics}}
      \end{table}

    \subsubsection{Preprocessing}
    \label{subsubsec:preprocessing}
      For each dataset, we apply the following preprocessing steps: sentence
      segmentation, word tokenization and Part-Of-Speech tagging. For word
      tokenization, we use the TreebankWordTokenizer provided by the python
      Natural Language ToolKit~\cite{bird2009nltk} for English and the Bonsai
      word tokenizer\footnote{The Bonsai word tokenizer is a tool provided with
      the Bonsai PCFG-LA parser:
      \url{http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html}.} for
      French. For part-of-speech tagging, we use the Stanford
      POS-tagger~\cite{toutanova2003stanfordpostagger} for English and
      MElt~\cite{denis2009melt} for French.

    \subsubsection{Evaluation Measures}
    \label{subsubsec:keyphrase_extraction_evaluation_measures}
      To quantify the capacity of the keyphrase candidate extraction methods to
      provide suitable candidates and avoid irrelevant ones, we compute the
      number of extracted candidates (Cand./Doc.) and confront it with the
      maximum recall (Rmax) that can be achieved. To do so, we compute a quality
      ratio (QR):
      \begin{align}
        \text{QR} &= \frac{\text{Rmax}}{\text{Cand./Doc.}}
      \end{align}
      The highest is the QR value of a candidate set, the best is its quality.

      The performance of the keyphrase extraction methods is expressed in terms
      of precision (P), recall (R) and f-score (f1-measure, F) when a maximum of
      10 keyphrases are extracted.

  \subsection{Candidate Extraction}
  \label{subsec:candidate_extraction}

    This section presents the intrinsic evaluation of the candidate extraction
    methods described in Section~\ref{sec:candidate_extraction}. The aim is to
    compare the methods in terms of quantity of extracted candidates and
    percentage of reference keyphrases that can be found in the best case
    (maximum recall).

    \subsubsection{Method Settings}
    \label{subsubsec:method_settings}
      The parameters of the methods are chosen to fit, as much as possible, the
      keyphrase properties inferred from the training sets (see
      Section~\ref{sec:definition_of_candidate_keyphrases}).

      According to Property~\ref{prop:informativity}, we try two \textbf{n-gram
      extraction} methods with low $n$ values: the first method extracts
      filtered n-grams where $n = \{1..2\}$ and the second method extracts
      filtered n-grams where $n = \{1..3\}$. The stop words used for the
      filtering are part of the IR Multilingual
      Resources\footnote{\url{http://members.unine.ch/jacques.savoy/clef/index.html}}
      provided by the University of Neuchâtel (UniNE).

      We define two \textbf{pattern matching} methods. The first one learns the
      keyphrase POS tag patterns from the training documents and extracts
      textual units that match one of the patterns. The second method follows
      both Property~\ref{prop:noun_phrases} and previous work by looking for
      the longest sequences of nouns and
      adjectives~\cite{hassan2010conundrums}, namely the longest NPs.

      The \textbf{NP-chunk extraction} is performed using pattern matching. Only
      basic patterns are used:
      \begin{itemize}
        \item{Np+ $|$~(A+~Nc) $|$~Nc+, for English;}
        \item{Np+ $|$~(A?~Nc~A+) $|$~(A~Nc) $|$~Nc+, for French.}
      \end{itemize}

%      The \textbf{sub-compounding} method of
%      \newcite{evans1996nounphraseanalysis} requires complex noun phrases. For
%      comparison purpose, we input the longest NPs extracted by pattern
%      matching (see above) and assume that they are complex NPs.

      The \textbf{terminalogical phrases} are extracted with the TermSuite
      tool~\cite{rocheteau2011termsuite}, using its default settings. TermSuite
      implements the state-of-the-art method for term extraction and term
      variant detection. TermSuite has extracted three terminologies from the
      training sets (30,807 terms for DUC, 76,597 terms for SemEval and 12,3796
      terms for DEFT). The textual units of a document that appears within the
      terminology are extracted as keyphrase candidates.

    \subsubsection{Result Analysis}
    \label{subsubsec:candidate_extraction_result_analysis}
      Table~\ref{tab:candidate_extraction_statistics} shows the number of
      textual units extracted by each candidate extraction method and confronts
      it to the maximum recall that can be achieved.
%    \begin{table*}
%      \centering
%      \begin{tabular}{rcccccc}
%        \toprule
%        \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{2}{c}{\textbf{DUC}} & \multicolumn{2}{c}{\textbf{SemEval}} & \multicolumn{2}{c}{\textbf{DEFT}}\\
%        \cmidrule(r){2-3}\cmidrule(lr){4-5}\cmidrule(l){6-7}
%        & Candidates & Rmax & Candidates & Rmax & Candidates & Rmax\\
%        \midrule
%        \{1..2\}-grams & $~~$49098 & 76.6 & 163358 & 61.0 & 238678 & 67.3\\
%        \{1..3\}-grams & $~~$59623 & \textbf{90.8} & 258054 & \textbf{72.2} & 378526 & 74.1\\
%        %\{1..4\}-grams & $~~$78024 & 92.6 & 365151 & 74.1 & 533753 & 78.2\\
%        Learned patterns & $~~$31764 & 90.6 & 122741 & 69.8 & 199789 & \textbf{76.5}\\
%        Longest NPs & $~~$15559 & 88.7 & $~~$64649 & 62.4 & $~~$85047 & 61.1\\
%        NP-chunks & $~~$14994 & 76.0 & $~~$59839 & 56.6 & $~~$75548 & 63.0\\
%        Sub-compounds & $~~$17181 & 90.6 & $~~$71224 & 64.4 & $~~$86866 & 61.1\\
%        Acabit & $~~~~$2377 & 26.7 & $~~$13214 & 17.6 & $~~$11106 & 13.4\\
%        TermSuite & $~~$16173 & 46.1 & $~~$49449 & 32.3 & $~~$60162 & 52.8\\
%        \bottomrule
%      \end{tabular}
%      \caption{Candidate extraction statistics. Rmax stands for maximum recall,
%               i.e.~the percentage of candidates that match with reference
%               keyphrases. \label{tab:candidate_extraction_statistics}}
%    \end{table*}
    \begin{table*}
      \centering
      \begin{tabular}{@{~}r@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~}}
        \toprule
        \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
        \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule{8-10}
        & Cand./Doc. & Rmax & QR & Cand./Doc. & Rmax & QR & Cand./Doc. & Rmax &
        QR\\
        \midrule
        \{1..2\}-grams & $~~~$491.0 & 76.6 & 0.156 & 1,633.6 & 61.0 & 0.037 & 2,566.4 & 67.3 & 0.026\\
        \{1..3\}-grams & $~~~$596.2 & 90.8 & 0.152 & 2,580.5 & 72.2 & 0.028 & 4,070.2 & 74.1 & 0.018\\
        Learned patterns & $~~~$317.6 & 90.6 & 0.285 & 1,227.4 & 69.8 & 0.057 & 2,148.3 & 76.5 & 0.036\\
        Longest NPs & $~~~$155.6 & 88.7 & \textbf{0.570} & $~~~$646.5 & 62.4 & \textbf{0.097} & $~~~$914.5 & 61.1 & 0.067\\
        NP-chunks & $~~~$149.9 & 76.0 & 0.507 & $~~~$598.4 & 56.6 & 0.095 & $~~~$812.3 & 63.0 & 0.078\\
        %Sub-compounds & $~~$171.8 & 90.6 & $~~$712.2 & 64.4 & $~~$934.0 & 61.1\\
        %Acabit & $~~~~$23.8 & 26.7 & $~~$132.1 & 17.6 & $~~$119.4 & 13.4\\
        %Terms & $~~$161.7 & 46.1 & $~~$494.5 & 32.3 & $~~$646.9 & 52.8\\
        Terms & $~~~$161.8 & 46.1 & 0.285 & $~~~$498.6 & 32.4 & 0.065 & $~~~$647.0 & 52.8 & \textbf{0.082}\\
        \bottomrule
      \end{tabular}
      \caption{Candidate extraction statistics.
               \label{tab:candidate_extraction_statistics}}
    \end{table*}

      The extraction  of n-grams gives a huge amount of candidates and allows a
      near perfect maximum recall\footnote{According to the amount of missing
      keyphrases, the maximum recall that can be achieved is 97.2\% for DUC,
      87.9\% for SemEval and 88.9\% for DEFT.}, whereas the other extraction
      methods provide less candidates and allows a lower maximum recall.
      However, for the extraction of the longest NPs and the NP-chunks, the
      maximum recall does not significantly decrease compared to the number of
      candidates. According to the quality ratio, the lower is the size of a
      candidate sets and the highest is the maximum recall, the better is its
      quality. Hence, the methods extracting better
      candidates are the ones extracting the longest NPs and the NP-chunks,
      followed by TermSuite candidates (terms) and the candidates matching
      learned patterns. As expected, the n-gram extraction is too exhaustive
      and, therefore, provides an important amount of irrelevant candidates.
      
      We observe that the evolution of the quality of TermSuite candidates over
      the three datasets is not stable compared to the other candidate sets.
      Indeed, term extraction methods rely on the fact that every document of a
      dataset belong to the same domain. Hence, TermSuite candidates are more
      suitable for SemEval and DEFT, which contains domain specific documents
      (respectively Computer Sciences and Humanities and Social Sciences), than
      DUC.

%      TermSuite provides a few candidates and does not allow a good maximum
%      recall. However, terminological phrases are coherent with our definition
%      of a keyphrase and consistent with both
%      properties~\ref{prop:informativity} and~\ref{prop:noun_phrases}. If the
%      candidate set extracted by TermSuite has a high quality, the performance
%      of the keyphrase extraction may not suffer from the important drop of the
%      maximum recall. Figure~\ref{fig:candidate_intersections} shows the
%      intersections between the n-gram candidates and the term candidates. We
%      observe that n-grams cover almost all the terms, but n-grams that are not
%      within the terms might be unrelevant candidates. The experiments of
%      Section~\ref{subsec:keyphrase_extraction} can confirm whether or not terms
%      are suitable for the keyphrase extraction task.
%      \begin{figure*}
%        \begin{minipage}{.32\linewidth}
%          \centering
%          \subfigure[\textbf{DUC}\label{subfig1:candidate_intersections}]{
%            \begin{overpic}[height=.9\linewidth]{include/duc_refs_term_suite_1_2_3_grams.eps}
%              \put(5,45){$\{1..3\}$-grams}
%              \put(66,42){Terms}
%              \put(80,72){Refs}
%            \end{overpic}
%          }
%        \end{minipage}
%        \hfill
%        \begin{minipage}{.32\linewidth}
%          \centering
%          \subfigure[\textbf{SemEval}\label{subfig2:candidate_intersections}]{
%            \begin{overpic}[height=.9\linewidth]{include/semeval_refs_term_suite_1_2_3_grams.eps}
%              \put(7,45){$\{1..3\}$-grams}
%              \put(68,40){Terms}
%              \put(83,64){Refs}
%            \end{overpic}
%          }
%        \end{minipage}
%        \hfill
%        \begin{minipage}{.32\linewidth}
%          \centering
%          \subfigure[\textbf{DEFT}\label{subfig3:candidate_intersections}]{
%            \begin{overpic}[height=.9\linewidth]{include/deft_refs_term_suite_1_2_3_grams.eps}
%              \put(7,46){$\{1..3\}$-grams}
%              \put(70,45){Terms}
%              \put(88,61){Refs}
%            \end{overpic}
%          }
%        \end{minipage}
%        \hfill
%        \begin{minipage}{.32\linewidth}
%          \centering
%          \vspace{1em}
%          \subfigure[TermSuite candidates compared to longest noun phrases and \textbf{DUC} references.\label{subfig4:candidate_intersections}]{
%            \hspace{1.6em}
%            \begin{overpic}[height=.48\linewidth]{include/duc_refs_term_suite_longest_nps.eps}
%              \put(10,18){Longest}\put(19,6){NPs}
%              \put(43,45){TermSuite}
%              \put(38,74){Refs}
%            \end{overpic}
%            \hspace{1.6em}
%          }
%        \end{minipage}
%        \hfill
%        \begin{minipage}{.32\linewidth}
%          \centering
%          \vspace{.9em}
%          \subfigure[TermSuite candidates compared to longest noun phrases and \textbf{SemEval} references.\label{subfig5:candidate_intersections}]{
%            \hspace{1.8em}
%            \begin{overpic}[height=.48\linewidth]{include/semeval_refs_term_suite_longest_nps.eps}
%              \put(15,18){Longest}\put(24,6){NPs}
%              \put(45,45){TermSuite}
%              \put(48,75){Refs}
%            \end{overpic}
%            \hspace{1.8em}
%          }
%        \end{minipage}
%        \hfill
%        \begin{minipage}{.32\linewidth}
%          \centering
%          \vspace{1.6em}
%          \subfigure[TermSuite candidates compared to longest noun phrases and \textbf{DEFT} references.\label{subfig6:candidate_intersections}]{
%            \hspace{2.4em}
%            \begin{overpic}[height=.42\linewidth]{include/deft_refs_term_suite_longest_nps.eps}
%              \put(12,19){Longest}\put(23,5){NPs}
%              \put(40,45){TermSuite}
%              \put(48,75){Refs}
%            \end{overpic}
%            \hspace{2.4em}
%          }
%        \end{minipage}
%        \caption{Intersection of TermSuite candidates with $\{1..3\}$-grams
%                 % TODO update
%                 \label{fig:candidate_intersections}}
%      \end{figure*}

  \subsection{Keyphrase Extraction}
  \label{subsec:keyphrase_extraction}
    This section presents the extrinsic evaluation of the candidate extraction
    methods. The aim is to observe the impact of the candidate extraction
    methods on the keyphrase extraction task.

    \subsubsection{Result Analysis}
    \label{subsubsec:candidate_extraction_result_analysis}
      Tables~\ref{tab:tfidf_results},~\ref{tab:topicrank_results}~and~\ref{tab:kea_results}
      show the performance of respectively TF-IDF, TopicRank and KEA when
      they extract keyphrases from keyphrase candidates provided by each
      candidate extraction method.
      \begin{table*}
        \centering
        \begin{tabular}{@{~}rccccccccc@{~}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          \{1..2\}-grams & 14.7 & 19.5 & 16.5 & 10.3 & $~~$7.0 & $~~$8.3 & $~~$8.1 & 15.1 & 10.4\\
          \{1..3\}-grams & 14.3 & 19.0 & 16.1 & $~~$9.0 & $~~$6.0 & $~~$7.2 & $~~$6.7 & 12.5 & $~~$8.6\\
          %\{1..4\}-grams & 13.7 & 18.2 & 15.4 & $~~$8.4 & $~~$5.6 & $~~$6.7 & $~~$6.7 & 12.5 & $~~$8.6\\
          Learned patterns & 19.1 & 25.4 & 21.5 & 10.7 & $~~$7.3 & $~~$8.6 & $~~$7.0 & 13.1 & $~~$9.0\\
          Longest NPs & 24.2 & 31.7 & \textbf{27.0} & 11.7 & $~~$7.9 & $~~$9.3 & $~~$9.5 & 17.6 & 12.1\\
          NP-chunks & 21.1 & 28.1 & 23.8 & 11.9 & $~~$8.0 & \textbf{$~~$9.5} & $~~$9.6 & 17.9 & 12.3\\
          %Sub-compounds & 22.8 & 29.9 & 25.5 & 10.8 & $~~$7.2 & $~~$8.6 & $~~$9.2 & 17.2 & 11.9\\
          %Acabit & 15.3 & 19.6 & 17.0 & $~~$8.6 & $~~$6.1 & $~~$7.1 & $~~$2.4 & $~~$5.6 & $~~$3.3\\
          %Terms & 17.4 & 23.2 & 19.6 & 11.5 & $~~$8.3 & \textbf{$~~$9.5} & 11.3 & 21.1 & \textbf{14.5}\\
          Terms & 17.4 & 23.2 & 19.6 & 11.2 & $~~$8.1 & $~~$9.3 & 11.3 & 21.1 & \textbf{14.5}\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate extraction methods, when extracting 10
                 keyphrases with \textbf{TF-IDF}.
                 \label{tab:tfidf_results}}
      \end{table*}
      \begin{table*}
        \centering
        \begin{tabular}{@{~}rccccccccc@{~}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          \{1..2\}-grams & 10.2 & 14.1 & 11.7 & 11.9 & $~~$8.2 & $~~$9.6 & $~~$5.8 & 11.0 & $~~$7.5\\
          \{1..3\}-grams & $~~$7.8 & 10.7 & $~~$8.9 & $~~$9.5 & $~~$6.7 & $~~$7.7 & $~~$6.2 & 11.4 & $~~$8.0\\
          %\{1..4\}-grams & $~~$7.1 & $~~$9.7 & $~~$8.1 & & & & & & \\
          Learned patterns & 14.9 & 19.8 & 16.7 & 12.2 & $~~$8.5 & \textbf{$~~$9.9} & $~~$8.8 & 16.1 & 11.3\\
          Longest NPs & 17.7 & 23.2 & \textbf{19.8} & 11.6 & $~~$7.9 & $~~$9.3 & 11.6 & 21.5 & \textbf{14.9}\\
          NP-chunks & 13.3 & 21.5 & 18.3 & 11.7 & $~~$8.0 & $~~$9.4 & 11.1 & 20.7 & 14.4\\
          %Sub-compounds & 18.3 & 24.0 & \textbf{20.5} & 11.3 & $~~$7.7 & $~~$9.0 & 11.6 & 21.5 & \textbf{14.9}\\
          %Acabit & 11.2 & 14.2 & 12.3 & 10.2 & $~~$7.1 & $~~$8.3 & $~~$3.4 & $~~$7.9 & $~~$4.7\\
          %TermSuite & 10.4 & 13.9 & 11.7 & $~~$8.8 & $~~$6.4 & $~~$7.4 & $~~$9.6 & 18.5 & 12.4\\
          Terms & 10.4 & 13.9 & 11.7 & $~~$8.9 & $~~$6.5 & $~~$7.5 & $~~$9.6 & 18.5 & 12.4\\
          %Terms clusters & $~~$5.7 & $~~$7.9 & $~~$6.5 & $~~$6.4 & $~~$4.7 & $~~$5.4 & $~~$8.7 & 16.0 & 11.2\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate extraction methods, when extracting 10
                 keyphrases with \textbf{TopicRank}.
                 \label{tab:topicrank_results}}
      \end{table*}
      \begin{table*}
        \centering
        \begin{tabular}{@{~}rccccccccc@{~}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          \{1..2\}-grams & 12.3 & 17.1 & 14.1 & 19.2 & 13.6 & 15.8 & 13.1 & 24.5 & 16.9\\
          \{1..3\}-grams & 12.0 & 16.6 & 13.7 & 19.4 & 13.7 & 15.9 & 13.4 & 25.3 & 17.3\\
          %\{1..4\}-grams & 11.7 & 16.1 & 13.4 & 19.5 & 13.8 & 16.0 & 13.7 & 25.7 & 17.6\\
          Learned patterns & 12.9 & 17.8 & 14.8 & 19.6 & 13.8 & \textbf{16.1} & 14.7 & 27.6 & 19.0\\
          Longest NPs & 14.5 & 19.9 & \textbf{16.5} & 19.6 & 13.7 & 16.0 & 14.1 & 26.3 & 18.1\\
          NP-chunks & 13.5 & 18.6 & 15.4 & 19.5 & 13.7 & 16.0 & 14.3 & 26.8 & 18.4\\
          %Sub-compounds & 14.6 & 20.0 & \textbf{16.7} & 19.3 & 13.5 & 15.8 & 14.1 & 26.3 & 18.1\\
          %Acabit & 14.6 & 19.1 & 16.3 & 15.6 & 10.8 & 12.6 & $~~$4.7 & 10.5 & $~~$6.4\\
          %Terms & 12.5 & 17.2 & 14.3 & 13.8 & 10.0 & 11.5 & 14.7 & 28.0 & \textbf{19.1}\\
          Terms & 12.5 & 17.2 & 14.3 & 13.9 & 10.1 & 11.6 & 14.7 & 28.0 & \textbf{19.1}\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate extraction methods, when extracting 10
                 keyphrases with \textbf{KEA}.
                 \label{tab:kea_results}}
      \end{table*}
%      \begin{table*}
%        \centering
%        \begin{tabular}{rccccccccc}
%          \toprule
%          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
%          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
%          & P & R & F & P & R & F & P & R & F\\
%          \midrule
%          \{1..2\}-grams & 15.5 & 20.5 & 17.4 & 10.4 & $~~$7.0 & \textbf{$~~$8.3} & $~~$3.0 & $~~$6.2 & $~~$4.0\\
%          \{1..3\}-grams & 13.7 & 18.0 & 15.3 & $~~$3.4 & $~~$2.3 & $~~$2.7 & $~~$1.9 & $~~$4.2 & $~~$2.6\\
%          %\{1..4\}-grams & $~~$7.7 & 10.1 & $~~$8.6 & $~~$1.4 & $~~$1.0 & $~~$1.1 & $~~$1.1 & $~~$2.4 & $~~$1.5\\
%          Learned patterns & 18.7 & 24.3 & 20.8 & $~~$4.6 & $~~$3.1 & $~~$1.2 & $~~$1.9 & $~~$4.2 & $~~$2.6\\
%          Longest NPs & 22.8 & 29.5 & \textbf{25.3} & $~~$3.7 & $~~$2.5 & $~~$3.0 & $~~$4.6 & $~~$9.2 & $~~$6.1\\
%          NP-chunks & 20.6 & 27.3 & 23.1 & $~~$8.4 & $~~$5.7 & $~~$6.7 & $~~$4.9 & $~~$9.7 & $~~$6.4\\
%          Sub-compounds & 21.2 & 27.3 & 23.5 & $~~$3.5 & $~~$2.4 & $~~$2.8 & $~~$4.6 & $~~$9.2 & $~~$6.1\\
%          %Acabit & 15.3 & 19.8 & 17.0 & $~~$7.6 & $~~$5.2 & $~~$6.1 & $~~$2.9 & $~~$6.6 & $~~$4.0\\
%          TermSuite & & & & & & & & & \\
%          \bottomrule
%        \end{tabular}
%        \caption{Comparison of candidate extraction methods, when extracting 10
%                 keyphrases with \textbf{SingleRank}.
%                 \label{tab:singlerank_results}}
%      \end{table*}
      
      The keyphrase extraction methods themselves behave differently. KEA has
      stable results with every candidate extraction method while TF-IDF and
      TopicRank have results that improve when the quality of the candidate set
      improves.

      Globally, for every method the best performance is achieved when using
      pattern matching (candidates extracted from learned patterns, longest NPs
      and NP-chunks). This observation confirms the assumption that a small
      candidate set having a good quality is better than an exhaustive
      candidate set. In most cases, the longest NPs induce the best results,
      comforting their usage in previous
      work~\cite{wan2008expandrank,hassan2010conundrums,bougouin2013topicrank}.

      TermSuite candidates induce lower results than the pattern matching
      methods. However, the results are better than the ones obtained with
      n-grams. That point is important and confirms that the quality of a
      candidate set must prevail over its exhaustivity. Indeed, we can see in
      Figure~\ref{fig:quality_prevails_over_exhaustivity} that TermSuite
      candidates do not contain different positive candidates than the n-grams
      and are mostly included in the n-grams, proving that n-grams contains a
      huge amount of irrelevant candidates acting as noise that reduces the
      performance of the keyphrase extraction task. Finally, we observe
      competitive results with the pattern matching methods when the dataset
      belongs to a specific domain (DEFT or SemEval). Thus, terms are good
      candidates when the processed data belong to a known domain.
      \begin{figure}[t]
        \begin{center}
        \subfigure{
          \begin{overpic}[height=.06\linewidth]{include/duc_refs_term_suite_1_2_3_grams.eps}
            \put(-45,115){\textbf{DUC}}
          \end{overpic}
        }
        \subfigure{
          \begin{overpic}[height=.23\linewidth]{include/semeval_refs_term_suite_1_2_3_grams.eps}
            \put(10, 100){\textbf{SemEval}}
          \end{overpic}
        }
        \subfigure{
          \begin{overpic}[height=.45\linewidth]{include/deft_refs_term_suite_1_2_3_grams.eps}
            \put(35, 101){\textbf{DEFT}}
          \end{overpic}
        }
        \caption{Intersection of TermSuite candidates (gray), $\{1..3\}$-grams
                 (white) and reference keyphrases (black).
                 % TODO update
                 \label{fig:quality_prevails_over_exhaustivity}}
        \end{center}
      \end{figure}

\section{Conclusion}
\label{sec:conclusion}
  In this paper, we argue that the candidate extraction is the most critical
  step of the keyphrase extraction task and studied the impact of various
  candidate extractions over different keyphrase extraction methods.

  According to the reference keyphrases of three standard evaluation datasets,
  we inferred two general keyphrase properties. Among the candidate extraction
  methods, extracting textual units matching given patterns or NP-chunks are the
  best ways to obtain keyphrase candidates that fit both properties.

  To confirm our assertions, we conducted two experiments to compare the quality
  of the candidate sets and observe their impact on
  the performance on different keyphrase extraction methods. Experimental
  results show that unsupervised methods do not have stable results depending on
  the candidate extraction methods. We found that concise candidate sets
  allowing a high maximum recall induce better results than huge candidate sets
  allowing a not significantly highest maximum recall. In other words, the
  quality of a candidate set prevails over its exhaustivity.

  Our work also outlined the usage of terminalogical phrases (terms) as
  keyphrase candidates. However, these candidates suffer from the requirement
  that the treated documents must belong to a specific domain. On DEFT, which
  fits this requirement, two keyphrase extraction methods, over three, obtained
  better performances while using term candidates. As for the third method,
  TopicRank, we can try to improve the results by changing the ``naive
  clustering''~\cite{bougouin2013topicrank} for the grouping of terms and their
  variants performed by the TermSuite tool. This grouping can also be applied to
  other keyphrase extraction methods that use
  clustering~\cite{matsuo2004wordcooccurrence}.

  In future work, a deep analysis of the intersection between the candidates
  extracted by the presented candidate extraction methods could help to infer
  new keyphrase properties or see which methods could be combined.

